# Performance Comparison of Automated Trading Algorithms

## Literature Review/Technical Background/Competitor Analysis - 2

Except for original work, what we know about ZIP is largely based upon empirical studies that investigate its performance in a particular market. In the [report](1) published by R. Das, J. E. Hanson, J. O. Kephart, and G. Tesauro in 2001, it illustrated that ZIP agents outperformed human traders in all experiments. Due to the description in the report: The members of the IBM Watson Experimental Economics Laboratory (WEEL) developed GEM which is a special-purpose distributed system for experimental economics. Magenta is a prototype agent environment developed at IBM Resea. Das et al. developed a hybrid system combined GEM with Magenta for the experiments with humans and agents. In the experiments, two different time periods are used for the different algorithms: “fast” agents were defined as 1 second, and “slow” agents were defined as 5 seconds. The results presented in the experiments employed Gjerstad-Dickhaut (GD) fast agents and ZIP slow agents. The interviews with human subjects draw our attention to a weakness in the ZIP strategy. That is if using a specific non-optimal human strategy such as ‘fixed-profit-ratio’, the ZIP agents performed badly. However, Das et al. implemented a modification to the ZIP strategy. As Das et al. observe: `Preliminary results show that the modified ZIP agents retain high efficiency and are not easily misled by the fixed-profit-ratio agent.`. In conclusion, the agents utilized established algorithms, ZIP and GD, outperform non-expert human subjects. As the further work, Das et al. suggest testing their bidding agents against professional equities or commodities traders would assist to uncover more weaknesses in the strategies, thereby leading to significant improvements in the strategies.

In recent years, several studies have investigated the role of AA trader-agent strategy in the markets. A conference [paper](2) was published by J. Cartlidge, C. Szostek, M. De Luca and D. Cliff in 2012. They explored the performance of AA-ultra and AA-slow to investigate markets efficiency. AA-ultra is a fast trader-agents which interval time is 0.1s, and AA-slow is slow trader-agents which interval time is 10s and agents perform further internal calculations every 2.5s. For a market simulation environment, they applied a new series of artificial trading experiments using the OpEx experimental economics system first introduced at ICAART2011. Every time the market was reset before starting the experiment. Each experiment lasted 20 minutes. 3 buyers and 3 sellers each, a total of 6 human participants and 6 trader-agents traded in the market. AA-slow and AA-ultra were homogeneously configured as the trader-agents respectively. As the results presented through their experiments, Smith’s alpha values are lower for AA-slow experiments compared to AA-ultra in every replenishment cycle, indicating that slow trader-agents improve market convergence. In comparison with agents, humans perform better when trading against the slower trader-agents. Moreover, the profit dispersion of agents is similar under condition AA-slow and AA-ultra, yet the profit dispersion of humans and the market is apparently lower under condition AA-slow. Evidence from the study propose that slow trader-agents contribute a competitive equilibrium in the markets overall, and slow-agent markets have higher efficiency.

In a similar vein, M. De Luca and D. Cliff in 2015 [investigated](3) the performance of AA, ZIP, Gjerstad-Dickhaut (GD) and GDX in both humans vs. agent and agent vs. agent contexts. They used Open Exchange (OpEx), an experimental algorithmic trading platform, to simulate an electronic trading system. A primary characteristic of using OpEx framework is that multiple instances can be developed in Agent Host. In another word, multiple agents can be implemented at the same time, and each agent applies one specific algorithm and has its own configuration settings. Not only AA and ZIP but also GD and GDX strategies are explored in their study. The GD agent forms belief functions by using observed market data. GDX is a modified GD strategy, it employs Dynamic Programming (DP) to price orders. M. De Luca et al. made a similar build on the work of previous research. They implemented nine of human vs. agent experiments, three for each of ZIP, GDX and AA. Differ from previous work reported, they used 1s as the timing rule for the experiments, aim to simplify the comparison of the performances of the different trading agents. Overall, the agents performed better than humans. In addition, AA performed the best which gained 27% profit greater than humans. In pure agent vs. agent markets, AA, ZIP and GDX competitively traded against each other. In result, GDX beat ZIP, and AA defeated both GDX and ZIP. Moreover, the mean number of rounds per experiment won of AA is higher than GDX.

Much of the current literature on algorithmic trading systems have begun to pay particular attention to the appropriately simulated environment. Several previous research has established that AA offers the best performance of any published bidding strategy. However, [according](7) to Dave Cliff (2019), AA agents perform poorly in real-world markets. To date, most researchers investigating AA and ZIP have utilised Open Exchange (OpEx) or ExPo, whereas Bristol Stock Exchange (BSE) was applied in D, Cliff's research. Addition to this, MAA, a modified AA, was implemented to run in a LOB-based market. While, for the purpose of evaluating the performance of the original AA, only the microprice modification was applied in MAA. In the experiments, AA was against three other strategies: ZIC, ZIP, and a BSE built-in strategy SHVR. Moreover, trader-strategy ratios and supply and demand schedules (SDSs) are altered during the experiments. According to Vach (2015), ratios of the various strategies active in the market could heavily affect the measured performance of AA. To avoid this issue, various trading strategies with any given ratio was performed in the market, the performance of trading strategies was calculated summary statistics over a large number of trials collected. In previous studies, SDSs either were set as constant or went through one or more step-changes in the experiments. Apparently, it is not a natural behaviour in real-world markets. This behaviour was improved in Cliff's experimenters. Thanks to BSE, a look-up tables (LUTs) of real-world financial was used in experiments. A set of experiments were run where the market’s underlying equilibrium price was varied dynamically, aiming to explore whether AA dominates. The results of the experiments reveal that MAA performs significantly worse than either SHVR or ZIP in more realistic environments.


1. [R. Das, J. E. Hanson, J. O. Kephart, and G. Tesauro. Agent-human interactions in the continuous double auction. The Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI), Seattle, USA (August, 2001), 2001.](https://s3.amazonaws.com/academia.edu.documents/44417575/das.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1556512970&Signature=JT7dTpWP0YqQRQx1avBwaZBS3uk%3D&response-content-disposition=inline%3B%20filename%3DAgent-Human_Interactions_in_the_Continuo.pdf)
2. [TOO FAST – TOO FURIOUS: Faster Financial-Market Trading Agents Can Give Less Efficient Markets](https://www.researchgate.net/profile/John_Cartlidge/publication/273060607_Too_fast_too_furious_Faster_financial_market_trading_agents_can_give_less_efficient_markets/links/55882dfa08aeb29944448104.pdf)
3. [Evaluation of the “Adaptive-Aggressive” Trading-Agent Strategy Against Human Traders in CDA: AA Wins](https://www.researchgate.net/profile/Dave_Cliff/publication/267767159_Evaluation_of_the_Adaptive-Aggressive_Trading-Agent_Strategy_Against_Human_Traders_in_CDA_AA_Wins/links/54b791d40cf2bd04be33a4e5.pdf)
4. [Exhaustive Testing of Trader-agents in Realistically Dynamic Continuous Double Auction Markets: AA Does Not Dominate](local)